#!/usr/bin/env python3

from lib.game import Agent, RandomAgent
import sys
import random

class MinimaxAgent(RandomAgent):
    """An agent that makes decisions using the Minimax algorithm, using a
    evaluation function to approximately guess how good certain states
    are when looking far into the future.

    :param evaluation_function: The function used to make evaluate a
        GameState. Should have the parameters (state, player_id) where
        `state` is the GameState and `player_id` is the ID of the
        player to calculate the expected payoff for.

    :param alpha_beta_pruning: True if you would like to use
        alpha-beta pruning.

    :param max_depth: The maximum depth to search using the minimax
        algorithm, before using estimates generated by the evaluation
        function.
    """
    def __init__(self, evaluate_function, alpha_beta_pruning=False, max_depth=5):
        super().__init__()
        self.evaluate = evaluate_function
        self.alpha_beta_pruning = alpha_beta_pruning
        self.max_depth = max_depth

    def decide(self, state):
        # TODO: Implement this agent!
        #
        # Read the documentation in /src/lib/game/_game.py for
        # information on what the decide function does.
        #
        # Do NOT call the soccer evaluation function that you write
        # directly from this function! Instead, use
        # `self.evaluate`. It will behave identically, but will be
        # able to work for multiple games.
        #
        # Do NOT call any SoccerState-specific functions! Assume that
        # you can only see the functions provided in the GameState
        # class.
        #
        # If you would like to see some example agents, check out
        # `/src/lib/game/_agents.py`.

        if not self.alpha_beta_pruning:
            return self.minimax(state, state.current_player, self.max_depth)
        else:
            return self.minimax_with_ab_pruning(state, state.current_player, self.max_depth, -float('inf'), float('inf'))

    def minimax(self, state, player, depth=1):
        # This is the suggested method you use to do minimax.  Assume
        # `state` is the current state, `player` is the player that
        # the agent is representing (NOT the current player in
        # `state`!)  and `depth` is the current depth of recursion.

        print('Executing simple minimax')
        utility, move = self.max_value(state, player, depth)

        # return super().decide(state)
        return move
    
    def minimax_with_ab_pruning(self, state, player, depth=1,
                                alpha=-float('inf'), beta=float('inf')):
        # return super().decide(state)
        print('Executing ab pruning')
        utility, move = self.max_value(state, player, depth, alpha, beta)

        return move

    def max_value(self, state, player, depth=1, alpha=None, beta=None):

        #A "leaf" node is reached if the state is terminal or the depth level has been exhausted
        if (state.is_terminal != None or depth == 0):
            return self.evaluate(state, player), None

        #Placeholder variables for the best utility and move
        value = -float('inf')
        move = random.choice(state.actions)

        for action in state.actions:
            #Get the state that results from executing the "action" on the current state
            result_state = state.act(action)
            
            #Skip actions that return invalid states
            if (result_state is None):
                continue
            
            #Get utility/best-action from the tree level below(aka min level)
            value2, action2 = self.min_value(result_state, result_state.current_player, depth, alpha, beta)

            #Keep track of utility/action pair that has a higher utility.
            if (value2 > value):
                value, move = value2, action

                #If alpha-beta pruning is enabled, keep track of the highest utility so far 
                # through alpha 
                if (alpha != None):
                    alpha = max(alpha, value)
            
            #If alpha-beta pruning is enabhled, immediately return utility/action pair whose utility
            #happens less than beta. This indicates that the best utility has been found
            if (beta != None and beta >= value):
                return value, move

        #Return the best utility/action pair of this level.
        return value, move

    def min_value(self, state, player, depth=1, alpha=None, beta=None):

        #A "leaf" node is reached if the state is terminal or the depth level has been exhausted
        if (state.is_terminal != None or depth == 0):
            return self.evaluate(state, player), None

        #Placeholder variables for the best utility and move
        value = float('inf')
        move = random.choice(state.actions)
        
        for action in state.actions:
            result_state = state.act(action)
            
            #Skip actions that return invalid states
            if (result_state is None):
                continue
            
            #Get utility/best-action from the tree level below(aka max level)
            value2, action2 = self.max_value(result_state, result_state.current_player, depth-1, alpha, beta)

            #Keep track of utility/action pair that has a lower utility.
            if (value2 < value):
                value, move = value2, action

                if (beta != None):
                    beta = min(beta, value)
                
            if (alpha != None and value <= alpha):
                return value, move

        #Return the best utility/action pair of this level.
        return value, move

class MonteCarloAgent(RandomAgent):
    """An agent that makes decisions using Monte Carlo Tree Search (MCTS),
    using an evaluation function to approximately guess how good certain
    states are when looking far into the future.

    :param evaluation_function: The function used to make evaluate a
        GameState. Should have the parameters (state, player_id) where
        `state` is the GameState and `player_id` is the ID of the
        player to calculate the expected payoff for.

    :param max_playouts: The maximum number of playouts to perform
        using MCTS.
    """
    def __init__(self, evaluate_function, max_playouts=100):
        super().__init__()
        self.evaluate = evaluate_function
        self.max_playouts = max_playouts

    def decide(self, state):
        # TODO: Implement this agent!
        #
        # Read the documentation in /src/lib/game/_game.py for
        # information on what the decide function does.
        #
        # Do NOT call the soccer evaluation function that you write
        # directly from this function! Instead, use
        # `self.evaluate`. It will behave identically, but will be
        # able to work for multiple games.
        #
        # Do NOT call any SoccerState-specific functions! Assume that
        # you can only see the functions provided in the GameState
        # class.
        #
        # If you would like to see some example agents, check out
        # `/src/lib/game/_agents.py`.

        return self.monte_carlo(state, state.current_player)

    def monte_carlo(self, state, player):
        # This is the suggested method you use to do MCTS.  Assume
        # `state` is the current state, `player` is the player that
        # the agent is representing (NOT the current player in
        # `state`!).
        return super().decide(state)

